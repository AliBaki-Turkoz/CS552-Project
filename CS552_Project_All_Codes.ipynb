{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to C:\\Users\\alibaki.turkoz\\Desktop\\scopus_data_deneme.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    " \n",
    "# Chrome seçeneklerini ayarlayın\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(r\"--user-data-dir=C:\\Users\\alibaki.turkoz\\AppData\\Local\\Google\\Chrome\\User Data\\Default\")\n",
    "options.add_argument(\"--profile-directory=Profile 3\")\n",
    " \n",
    "# Chrome WebDriver başlatma\n",
    "driver = webdriver.Chrome(options=options)\n",
    " \n",
    "# URL'ler\n",
    "urls = [\n",
    "\"https://www.scopus.com/record/display.uri?eid=2-s2.0-85204581005&origin=resultslist&sort=plf-f&src=s&sid=32f0b224ece73acf734cfbc58c66530e&sot=aff&sdt=aff&s=AF-ID%2860014930%29+AND+SUBJAREA%28ENGI%29&sl=34&sessionSearchId=32f0b224ece73acf734cfbc58c66530e&relpos=0\"    # Diğer URL'ler burada devam ediyor...\n",
    "]\n",
    " \n",
    "# Excel'e kaydedilecek veriler\n",
    "data = []\n",
    " \n",
    "# URL'leri gezerek verileri toplayalım\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Sayfanın yüklenmesi için bekleyin\n",
    " \n",
    "    # Verileri çekmek için XPath'ler\n",
    "    try:\n",
    "        # Makale ismini çekme\n",
    "        article_title = driver.find_element(By.XPATH, '//*[@id=\"doc-details-page-container\"]/article/div[2]/div[2]/section/div[1]/div[1]/div/h2/span').text\n",
    "       \n",
    "        # Yazar isimlerini çekme\n",
    "        authors = driver.find_element(By.XPATH, '//*[@id=\"doc-details-page-container\"]/article/div[2]/div[2]/section/div[1]/div[2]/div/ul').text\n",
    "       \n",
    "        # Üniversite adlarını çekme\n",
    "        affiliations = driver.find_element(By.XPATH, '//*[@id=\"affiliation-section\"]/div/div/ul').text\n",
    " \n",
    "        # Veriyi listeye ekleyelim\n",
    "        data.append([article_title, authors, affiliations, url])\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data from {url}: {e}\")\n",
    " \n",
    "# Veriyi DataFrame'e çevirme\n",
    "df = pd.DataFrame(data, columns=[\"Article Title\", \"Authors\", \"Affiliations\", \"URL\"])\n",
    " \n",
    "# Excel dosyasına kaydetme\n",
    "file_path = r\"C:\\Users\\alibaki.turkoz\\Desktop\\scopus_data_deneme.xlsx\"\n",
    "df.to_excel(file_path, index=False)\n",
    " \n",
    "# Tarayıcıyı kapat\n",
    "driver.quit()\n",
    " \n",
    "print(f\"Data has been saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV TO EXCEL CLEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel dosyası başarıyla kaydedildi: C:\\Users\\alibaki.turkoz\\Downloads\\scopus.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "# CSV dosyasının yolu\n",
    "csv_file_path = r\"C:\\Users\\alibaki.turkoz\\Downloads\\scopus.csv\"  # Buraya .csv dosyasının yolunu yazın\n",
    "excel_file_path = r\"C:\\Users\\alibaki.turkoz\\Downloads\\scopus.xlsx\"   # Çıkış dosyasının adı\n",
    " \n",
    "# CSV dosyasını yükleme\n",
    "df = pd.read_csv(csv_file_path)\n",
    " \n",
    "# İlgili sütunları seçme\n",
    "columns_to_extract = [\"Author full names\", \"Document Type\", \"Title\", \"Year\",\"Affiliations\"]\n",
    "selected_columns_df = df[columns_to_extract]\n",
    " \n",
    "# Seçilen sütunları yeni bir Excel dosyasına kaydetme\n",
    "selected_columns_df.to_excel(excel_file_path, index=False)\n",
    " \n",
    "print(f\"Excel dosyası başarıyla kaydedildi: {excel_file_path}\")\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasördeki tüm csv leri alır ve tek excel de kaydeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (0).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (1).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (2).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (3).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (4).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (5).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (6).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (7).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (8).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (9).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (10).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (11).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (12).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (13).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (14).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (15).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (16).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (17).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (18).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (19).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (20).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (21).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (22).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (23).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (24).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (25).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (26).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (27).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (28).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (29).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (30).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (31).csv\n",
      "İşleniyor: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus (32).csv\n",
      "Bütün dosyalar başarıyla birleştirildi ve kaydedildi: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus_articles_1854_2015.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Klasör yolu (buraya klasörünüzün yolunu yazın)\n",
    "folder_path = r\"C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\"\n",
    "output_excel_path = r\"C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ\\Scopus Articles 1854-2015 - Kopya\\scopus_articles_1854_2015.xlsx\"\n",
    "\n",
    "# Birleştirme için boş bir DataFrame oluştur\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Dosyaları sırayla işlemek için döngü\n",
    "for i in range(33):  # 0'dan 32'ye kadar döngü\n",
    "    file_name = f\"scopus ({i}).csv\"  # Dosya adını oluştur\n",
    "    file_path = f\"{folder_path}\\\\{file_name}\"  # Tam yolu oluştur\n",
    "    \n",
    "    try:\n",
    "        # CSV dosyasını oku\n",
    "        print(f\"İşleniyor: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Seçilen sütunları ayıkla\n",
    "        columns_to_extract = [\"Author full names\", \"Document Type\", \"Title\", \"Year\", \"Affiliations\"]\n",
    "        selected_columns_df = df[columns_to_extract]\n",
    "        \n",
    "        # Birleştir\n",
    "        merged_df = pd.concat([merged_df, selected_columns_df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Dosya bulunamadı: {file_path}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Sütun eksikliği nedeniyle hata oluştu: {file_name}, Eksik sütun: {e}\")\n",
    "\n",
    "# Tek bir Excel dosyasına kaydet\n",
    "merged_df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "print(f\"Bütün dosyalar başarıyla birleştirildi ve kaydedildi: {output_excel_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia dan İllerdeki Üni isimlerini çeker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected from https://tr.wikipedia.org/wiki/Antalya%27daki_%C3%BCniversiteler_listesi\n",
      "Data has been saved to C:\\Users\\alibaki.turkoz\\Desktop\\universities_in_Antalya.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    " \n",
    "# Chrome seçeneklerini ayarlayın\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(r\"--user-data-dir=C:\\Users\\alibaki.turkoz\\AppData\\Local\\Google\\Chrome\\User Data\\Default\")\n",
    "options.add_argument(\"--profile-directory=Profile 2\")\n",
    " \n",
    "# Chrome WebDriver başlatma\n",
    "driver = webdriver.Chrome(options=options)\n",
    " \n",
    "# Wikipedia URL'si\n",
    "url = \"https://tr.wikipedia.org/wiki/Antalya%27daki_%C3%BCniversiteler_listesi\"\n",
    " \n",
    "# Excel'e kaydedilecek veriler\n",
    "data = []\n",
    " \n",
    "# URL'yi açalım\n",
    "driver.get(url)\n",
    "time.sleep(2)  # Sayfanın yüklenmesi için bekleyin\n",
    " \n",
    "# Verileri çekmek için XPath'ler\n",
    "try:\n",
    "    # Üniversiteler tablosundaki il ve üniversite adlarını çekelim\n",
    "    rows = driver.find_elements(By.XPATH, '//*[@id=\"mw-content-text\"]/div[1]/table[2]/tbody/tr')\n",
    "\n",
    "    for row in rows:\n",
    "        columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        if len(columns) > 1:  # Boş satırlardan kaçın\n",
    "            city = columns[0].text.strip()  # İl adı\n",
    "            university_name = columns[1].text.strip()  # Üniversite adı\n",
    "            data.append([city, university_name])\n",
    " \n",
    "    print(f\"Data collected from {url}\")\n",
    " \n",
    "except Exception as e:\n",
    "    print(f\"Error extracting data from {url}: {e}\")\n",
    " \n",
    "# Veriyi DataFrame'e çevirme\n",
    "df = pd.DataFrame(data, columns=[\"City\", \"University Name\"])\n",
    " \n",
    "# Excel dosyasına kaydetme\n",
    "file_path = r\"C:\\Users\\alibaki.turkoz\\Desktop\\universities_in_Antalya.xlsx\"\n",
    "df.to_excel(file_path, index=False)\n",
    " \n",
    "# Tarayıcıyı kapat\n",
    "driver.quit()\n",
    " \n",
    "print(f\"Data has been saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected for Antalya from https://tr.wikipedia.org/wiki/Antalya%27daki_%C3%BCniversiteler_listesi\n",
      "Data has been saved to C:\\Users\\alibaki.turkoz\\Desktop\\universities_in_Antalya.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Chrome seçeneklerini ayarlayın\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(r\"--user-data-dir=C:\\Users\\alibaki.turkoz\\AppData\\Local\\Google\\Chrome\\User Data\\Default\")\n",
    "options.add_argument(\"--profile-directory=Profile 2\")\n",
    "\n",
    "# Chrome WebDriver başlatma\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Kullanıcıdan URL alın\n",
    "url = input(\"Enter the Wikipedia URL for the city: \")\n",
    "\n",
    "# Excel'e kaydedilecek veriler\n",
    "data = []\n",
    "\n",
    "# URL'yi açalım\n",
    "driver.get(url)\n",
    "time.sleep(2)  # Sayfanın yüklenmesi için bekleyin\n",
    "\n",
    "try:\n",
    "    # Şehir adını başlıktan al ve ekleri temizle\n",
    "    raw_city_name = driver.find_element(By.XPATH, '//*[@id=\"firstHeading\"]').text\n",
    "    city_name = raw_city_name.split(\"'\")[0].strip()  # \"'daki\" veya \"'deki\" gibi ekleri temizler\n",
    "\n",
    "    # Üniversiteler tablosundaki satırları bul\n",
    "    rows = driver.find_elements(By.XPATH, '//*[@id=\"mw-content-text\"]/div[1]/table[1]/tbody/tr')\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            # Üniversite adını al\n",
    "            university_name = row.find_element(By.XPATH, './td[1]/a').text.strip()\n",
    "            data.append([city_name, university_name])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {e}\")\n",
    "\n",
    "    print(f\"Data collected for {city_name} from {url}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting data from {url}: {e}\")\n",
    "\n",
    "# Veriyi DataFrame'e çevirme\n",
    "df = pd.DataFrame(data, columns=[\"City\", \"University Name\"])\n",
    "\n",
    "# Excel dosyasına kaydetme\n",
    "file_path = rf\"C:\\Users\\alibaki.turkoz\\Desktop\\universities_in_{city_name}.xlsx\"\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "# Tarayıcıyı kapat\n",
    "driver.quit()\n",
    "\n",
    "print(f\"Data has been saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YÖK ATLAS DAN ÜNİ SAYFASINDAN BÖLÜMLERİ ÇEKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected from https://yokatlas.yok.gov.tr/lisans-univ.php?u=2077\n",
      "Data has been saved to C:\\Users\\alibaki.turkoz\\Desktop\\Gaziantep_Sanko_University_Departments.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "import re  # Regular expression kullanacağız\n",
    " \n",
    "# Chrome seçeneklerini ayarlayın\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(r\"--user-data-dir=C:\\Users\\alibaki.turkoz\\AppData\\Local\\Google\\Chrome\\User Data\\Default\")\n",
    "options.add_argument(\"--profile-directory=Profile 2\")\n",
    " \n",
    "# Chrome WebDriver başlatma\n",
    "driver = webdriver.Chrome(options=options)\n",
    " \n",
    "# Düzeltilmiş URL\n",
    "url = \"https://yokatlas.yok.gov.tr/lisans-univ.php?u=2077\"\n",
    " \n",
    "# Excel'e kaydedilecek veriler\n",
    "data = set()  # Set kullanarak benzersiz veriler tutalım\n",
    " \n",
    "# URL'yi açalım\n",
    "driver.get(url)\n",
    "time.sleep(2)  # Sayfanın yüklenmesi için bekleyin\n",
    " \n",
    "# Veriyi çekmek için XPath\n",
    "try:\n",
    "    # Belirttiğiniz XPath kullanarak veriyi çekelim\n",
    "    element_xpath = '/html/body/div[1]/div[2]/div[2]/div'\n",
    " \n",
    "    # Veriyi bul ve metni al\n",
    "    element = driver.find_element(By.XPATH, element_xpath)\n",
    "    rows = element.text.strip().split('\\n')  # Satırlara ayıralım\n",
    " \n",
    "    # EA, SAY, DİL, SÖZ kelimelerini ve parantez içeriğini hariç tutarak verileri ekleyelim\n",
    "    for row in rows:\n",
    "        if not any(keyword in row for keyword in [\"EA\", \"SAY\", \"DİL\", \"SÖZ\", \"KKTC\"]):\n",
    "            # Parantez içindeki metni çıkaralım\n",
    "            row_cleaned = re.sub(r'\\(.*?\\)', '', row)  # Parantez içindekileri temizle\n",
    "            data.add(row_cleaned.strip())  # Satırı set'e ekleyelim (benzersiz olacak)\n",
    " \n",
    "    print(f\"Data collected from {url}\")\n",
    " \n",
    "except Exception as e:\n",
    "    print(f\"Error extracting data from {url}: {e}\")\n",
    " \n",
    "# Veriyi DataFrame'e çevirme\n",
    "df = pd.DataFrame(list(data), columns=[\"Departments\"])\n",
    " \n",
    "# Excel dosyasına kaydetme\n",
    "file_path = r\"C:\\Users\\alibaki.turkoz\\Desktop\\Gaziantep_Sanko_University_Departments.xlsx\"\n",
    "df.to_excel(file_path, index=False, header=True)\n",
    " \n",
    "# Tarayıcıyı kapat\n",
    "driver.quit()\n",
    " \n",
    "print(f\"Data has been saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# akademik.yok.gov.tr den üniversitedeki hoca isimlerini çeker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1...\n",
      "Processing page 2...\n",
      "Processing page 3...\n",
      "Processing page 4...\n",
      "Processing page 5...\n",
      "Processing page 6...\n",
      "Processing page 7...\n",
      "Processing page 8...\n",
      "Error processing pages: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div/div[2]/div[2]/div[2]/ul/li[9]/a\"}\n",
      "  (Session info: chrome=131.0.6778.86); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF713A96CB5+28821]\n",
      "\t(No symbol) [0x00007FF713A03840]\n",
      "\t(No symbol) [0x00007FF7138A578A]\n",
      "\t(No symbol) [0x00007FF7138F91BE]\n",
      "\t(No symbol) [0x00007FF7138F94AC]\n",
      "\t(No symbol) [0x00007FF713942647]\n",
      "\t(No symbol) [0x00007FF71391F33F]\n",
      "\t(No symbol) [0x00007FF71393F412]\n",
      "\t(No symbol) [0x00007FF71391F0A3]\n",
      "\t(No symbol) [0x00007FF7138EA778]\n",
      "\t(No symbol) [0x00007FF7138EB8E1]\n",
      "\tGetHandleVerifier [0x00007FF713DCFCAD+3408013]\n",
      "\tGetHandleVerifier [0x00007FF713DE741F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF713DDB5FD+3455453]\n",
      "\tGetHandleVerifier [0x00007FF713B5BDBB+835995]\n",
      "\t(No symbol) [0x00007FF713A0EB5F]\n",
      "\t(No symbol) [0x00007FF713A0A814]\n",
      "\t(No symbol) [0x00007FF713A0A9AD]\n",
      "\t(No symbol) [0x00007FF7139FA199]\n",
      "\tBaseThreadInitThunk [0x00007FF99A717374+20]\n",
      "\tRtlUserThreadStart [0x00007FF99B8DCC91+33]\n",
      "\n",
      "Data has been saved to C:\\Users\\alibaki.turkoz\\Desktop\\Gaziantep_Sanko_University_academic_staff.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    " \n",
    "# Chrome seçeneklerini ayarlayın\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(r\"--user-data-dir=C:\\Users\\alibaki.turkoz\\AppData\\Local\\Google\\Chrome\\User Data\\Default\")\n",
    "options.add_argument(\"--profile-directory=Profile 2\")\n",
    " \n",
    "# Chrome WebDriver başlatma\n",
    "driver = webdriver.Chrome(options=options)\n",
    " \n",
    "# Başlangıç URL'si\n",
    "url = \"https://akademik.yok.gov.tr/AkademikArama/view/universityListview.jsp\"\n",
    "driver.get(url)\n",
    "time.sleep(0.5)  # Sayfanın yüklenmesi için bekleyin\n",
    " \n",
    "# İkinci URL'ye geçiş yapmak için düğmeye tıklama\n",
    "try:\n",
    "    second_url_xpath = '/html/body/div/div[2]/div/table/tbody/tr[178]/td[1]/a'  # Düğme XPath\n",
    "    button = driver.find_element(By.XPATH, second_url_xpath)\n",
    "    button.click()\n",
    "    time.sleep(0.5)  # Yeni sayfanın yüklenmesini bekleyin\n",
    "except Exception as e:\n",
    "    print(f\"Error clicking the button to navigate: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    " \n",
    "# Tüm sayfalardan sonuçları toplamak için liste\n",
    "all_data = []\n",
    " \n",
    "# Sayfa gezintisi (toplam 60 sayfa)\n",
    "def get_page_xpath(page_number):\n",
    "    if page_number <= 10:\n",
    "        return f\"/html/body/div/div[2]/div[2]/div[2]/ul/li[{page_number}]/a\"\n",
    "    elif (page_number - 1) % 10 == 0:  # 10, 20, 30 gibi \"Next Page\" düğmesi\n",
    "        if page_number == 11:\n",
    "            return \"/html/body/div/div[2]/div[2]/div[2]/ul/li[11]/a\"  # 10'dan 11'e geçiş\n",
    "        else:\n",
    "            return \"/html/body/div/div[2]/div[2]/div[2]/ul/li[12]/a\"  # 20'den 21, 30'dan 31 geçiş\n",
    "    else:\n",
    "        inner_page_number = (page_number - 1) % 10 + 2\n",
    "        return f\"/html/body/div/div[2]/div[2]/div[2]/ul/li[{inner_page_number}]/a\"\n",
    " \n",
    "# Sayfa gezintisi (60 sayfa)\n",
    "try:\n",
    "    for page in range(1, 161):  # 1'den 60'a kadar olan sayfalar\n",
    "        print(f\"Processing page {page}...\")\n",
    "       \n",
    "        # Sayfanın tamamen yüklendiğinden emin olmak için bekleyin\n",
    "        time.sleep(1)\n",
    " \n",
    "        # Sayfadaki tüm yazarların bilgilerini bul\n",
    "        rows = driver.find_elements(By.XPATH, '//tr[starts-with(@id, \"authorInfo_\")]')\n",
    " \n",
    "        for row in rows:\n",
    "            try:\n",
    "                # Unvan XPath\n",
    "                title_xpath = './/td[3]/h6[1]'\n",
    "                title = row.find_element(By.XPATH, title_xpath).text.strip()\n",
    " \n",
    "                # İsim XPath\n",
    "                name_xpath = './/td[3]/h4/a'\n",
    "                name = row.find_element(By.XPATH, name_xpath).text.strip()\n",
    " \n",
    "                # Bölüm XPath\n",
    "                department_xpath = './/td[3]/h6[2]'\n",
    "                try:\n",
    "                    department = row.find_element(By.XPATH, department_xpath).text.strip()\n",
    "                except:\n",
    "                    department = \"Bölüm bilgisi yok\"\n",
    " \n",
    "                # Veriyi listeye ekle (Unvan ve isim birleştirilmiş, bölüm ayrı)\n",
    "                all_data.append([f\"{title} {name}\", department])\n",
    " \n",
    "            except Exception as row_error:\n",
    "                print(f\"Error processing row: {row_error}\")\n",
    " \n",
    "        # Sayfayı ilerlet\n",
    "        if page < 160:  # 60. sayfada yönlendirme yapılmaz\n",
    "            next_page_xpath = get_page_xpath(page + 1)\n",
    "            next_page_button = driver.find_element(By.XPATH, next_page_xpath)\n",
    "            next_page_button.click()\n",
    "            time.sleep(1)  # Sayfanın yüklenmesi için bekleyin\n",
    " \n",
    "except Exception as e:\n",
    "    print(f\"Error processing pages: {e}\")\n",
    " \n",
    "# Veriyi DataFrame'e çevirme\n",
    "df = pd.DataFrame(all_data, columns=[\"Name and Title\", \"Department\"])\n",
    " \n",
    "# Excel dosyasına kaydetme\n",
    "file_path = r\"C:\\Users\\alibaki.turkoz\\Desktop\\Gaziantep_Sanko_University_academic_staff.xlsx\"\n",
    "df.to_excel(file_path, index=False, header=True)\n",
    " \n",
    "print(f\"Data has been saved to {file_path}\")\n",
    " \n",
    "# Tarayıcıyı kapat\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARÇALI ÇEKİYOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1...\n",
      "Processing page 2...\n",
      "Processing page 3...\n",
      "Processing page 4...\n",
      "Processing page 5...\n",
      "Processing page 6...\n",
      "Processing page 7...\n",
      "Processing page 8...\n",
      "Processing page 9...\n",
      "Processing page 10...\n",
      "All data has been saved to C:\\Users\\alibaki.turkoz\\Desktop\\İzmir_Yüksek_Teknoloji_University_academic_staff.xlsx\n"
     ]
    }
   ],
   "source": [
    "### İLK 10 SAYFAYI ÇEKER\n",
    "## KULLANMAK İÇİN ÖNCE akademik.yok.gov.tr'nin URL Yİ KOPYALA YAPIŞTIR.\n",
    "# SONRA ÜNİVERSİTENİN ÜZERİNE GEL XPATH İNİ KOPYALA VE second url KISMINA YAPIŞTIR.\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Chrome seçeneklerini ayarlayın\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(r\"--user-data-dir=C:\\Users\\alibaki.turkoz\\AppData\\Local\\Google\\Chrome\\User Data\\Default\")\n",
    "options.add_argument(\"--profile-directory=Profile 2\")\n",
    "\n",
    "# Chrome WebDriver başlatma\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Başlangıç URL'si\n",
    "url = \"https://akademik.yok.gov.tr/AkademikArama/view/universityListview.jsp\"\n",
    "\n",
    "# URL'yi açalım\n",
    "driver.get(url)\n",
    "time.sleep(2)  # Sayfanın yüklenmesi için bekleyin\n",
    "\n",
    "# İkinci URL'ye geçiş yapmak için düğmeye tıklama\n",
    "try:\n",
    "    second_url_xpath = '/html/body/div/div[2]/div/table/tbody/tr[11]/td[1]/a'  # Düğme XPath\n",
    "    button = driver.find_element(By.XPATH, second_url_xpath)\n",
    "    button.click()\n",
    "    time.sleep(3)  # Yeni sayfanın yüklenmesini bekleyin\n",
    "except Exception as e:\n",
    "    print(f\"Error clicking the button to navigate: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Tüm sayfalardan sonuçları toplamak için liste\n",
    "all_data = []\n",
    "\n",
    "try:\n",
    "    # 1'den 10'a kadar olan sayfaları gez\n",
    "    for page in range(1, 11):  # 1'den 10'a kadar olan sayfalar\n",
    "        print(f\"Processing page {page}...\")\n",
    "\n",
    "        # Sayfadaki tüm yazarların bilgilerini bul\n",
    "        rows = driver.find_elements(By.XPATH, '//tr[starts-with(@id, \"authorInfo_\")]')\n",
    "\n",
    "        for row in rows:\n",
    "            try:\n",
    "                # Unvan XPath\n",
    "                title_xpath = './/td[3]/h6[1]'\n",
    "                title = row.find_element(By.XPATH, title_xpath).text.strip()\n",
    "\n",
    "                # İsim XPath\n",
    "                name_xpath = './/td[3]/h4/a'\n",
    "                name = row.find_element(By.XPATH, name_xpath).text.strip()\n",
    "\n",
    "                # Bölüm XPath\n",
    "                department_xpath = './/td[3]/h6[2]'\n",
    "                try:\n",
    "                    department = row.find_element(By.XPATH, department_xpath).text.strip()\n",
    "                except:\n",
    "                    department = \"Bölüm bilgisi yok\"\n",
    "\n",
    "                # Veriyi listeye ekle (Unvan ve isim birleştirilmiş, bölüm ayrı)\n",
    "                all_data.append([f\"{title} {name}\", department])\n",
    "\n",
    "            except Exception as row_error:\n",
    "                print(f\"Error processing row: {row_error}\")\n",
    "\n",
    "        # Sayfayı ilerlet\n",
    "        if page < 10:  # Eğer son sayfa değilse\n",
    "            try:\n",
    "                next_page_xpath = f'/html/body/div/div[2]/div[2]/div[2]/ul/li[{page + 1}]/a'\n",
    "                next_button = driver.find_element(By.XPATH, next_page_xpath)\n",
    "                next_button.click()\n",
    "                time.sleep(2)  # Sayfanın yüklenmesi için bekle\n",
    "            except Exception as next_error:\n",
    "                print(f\"Error navigating to page {page + 1}: {next_error}\")\n",
    "                break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during scraping: {e}\")\n",
    "\n",
    "# Veriyi DataFrame'e çevirme\n",
    "df = pd.DataFrame(all_data, columns=[\"Name and Title\", \"Department\"])\n",
    "\n",
    "# Excel dosyasına kaydetme\n",
    "file_path = r\"C:\\Users\\alibaki.turkoz\\Desktop\\İzmir_Yüksek_Teknoloji_University_academic_staff.xlsx\"\n",
    "df.to_excel(file_path, index=False, header=True)\n",
    "\n",
    "print(f\"All data has been saved to {file_path}\")\n",
    "\n",
    "# Tarayıcıyı kapat\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 KEZ İLERİ BASMAN GEREKİYOR SONRA 11 DEN İTİBAREN HEPSİNİ ÇEKER VE ÖNCEKİ İLK 10 SAYFANIN AÇTIĞI EXCEL DOSYASININ ALTINA KAYDEDER\n",
    "## KULLANMAK İÇİN ÖNCE akademik.yok.gov.tr'nin URL Yİ KOPYALA YAPIŞTIR.\n",
    "# SONRA ÜNİVERSİTENİN ÜZERİNE GEL XPATH İNİ KOPYALA VE second url KISMINA YAPIŞTIR.\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Chrome seçeneklerini ayarlayın\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(r\"--user-data-dir=C:\\Users\\alibaki.turkoz\\AppData\\Local\\Google\\Chrome\\User Data\\Default\")\n",
    "options.add_argument(\"--profile-directory=Profile 2\")\n",
    "\n",
    "# Chrome WebDriver başlatma\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Başlangıç URL'si\n",
    "url = \"https://akademik.yok.gov.tr/AkademikArama/view/universityListview.jsp\"\n",
    "\n",
    "# URL'yi açalım\n",
    "driver.get(url)\n",
    "time.sleep(2)  # Sayfanın yüklenmesi için bekleyin\n",
    "\n",
    "# İkinci URL'ye geçiş yapmak için düğmeye tıklama\n",
    "try:\n",
    "    second_url_xpath = '/html/body/div/div[2]/div/table/tbody/tr[124]/td[1]/a'  # Düğme XPath\n",
    "    button = driver.find_element(By.XPATH, second_url_xpath)\n",
    "    button.click()\n",
    "    time.sleep(3)  # Yeni sayfanın yüklenmesini bekleyin\n",
    "except Exception as e:\n",
    "    print(f\"Error clicking the button to navigate: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Tüm sayfalardan sonuçları toplamak için liste\n",
    "all_data = []\n",
    "\n",
    "try:\n",
    "    # Sayfa yüklenmeden önce 3 kez \"Sonraki\" düğmesine bas\n",
    "    for _ in range(0):  # 3 kez \"Sonraki\" düğmesine bas\n",
    "        try:\n",
    "            next_group_xpath = '/html/body/div/div[2]/div[2]/div[2]/ul/li[12]/a'  # \"Sonraki\" düğmesi XPath\n",
    "            next_group_button = driver.find_element(By.XPATH, next_group_xpath)\n",
    "            next_group_button.click()\n",
    "            time.sleep(3)  # Yeni sayfanın yüklenmesini bekle\n",
    "        except Exception as next_group_error:\n",
    "            print(f\"Error clicking 'Sonraki' button: {next_group_error}\")\n",
    "            break\n",
    "\n",
    "    # 12 döngü boyunca sayfalama işlemi yap\n",
    "    for _ in range(15):  # Bu döngü 12 kez tekrar edecek\n",
    "        print(f\"Processing page group...\")\n",
    "\n",
    "        # Sayfa 3'ten 11'e kadar olan sayfaları gez\n",
    "        for page in range(3, 13):  # 3'ten 11'e kadar olan sayfalar\n",
    "            print(f\"Processing page {page}...\")\n",
    "\n",
    "            # Sayfadaki tüm yazarların bilgilerini bul\n",
    "            rows = driver.find_elements(By.XPATH, '//tr[starts-with(@id, \"authorInfo_\")]')\n",
    "\n",
    "            for row in rows:\n",
    "                try:\n",
    "                    # Unvan XPath\n",
    "                    title_xpath = './/td[3]/h6[1]'\n",
    "                    title = row.find_element(By.XPATH, title_xpath).text.strip()\n",
    "\n",
    "                    # İsim XPath\n",
    "                    name_xpath = './/td[3]/h4/a'\n",
    "                    name = row.find_element(By.XPATH, name_xpath).text.strip()\n",
    "\n",
    "                    # Bölüm XPath\n",
    "                    department_xpath = './/td[3]/h6[2]'\n",
    "                    try:\n",
    "                        department_text = row.find_element(By.XPATH, department_xpath).text.strip()\n",
    "                        department = f\"{department_text}\"\n",
    "                    except:\n",
    "                        department = ''  # Bölüm yoksa boş bırak\n",
    "\n",
    "                    # Veriyi listeye ekle\n",
    "                    all_data.append([f\"{title} {name}\", department])\n",
    "\n",
    "                except Exception as row_error:\n",
    "                    print(f\"Error processing row: {row_error}\")\n",
    "\n",
    "            # Sayfayı ilerlet\n",
    "            if page < 12:  # Eğer son sayfa değilse\n",
    "                try:\n",
    "                    next_page_xpath = f'/html/body/div/div[2]/div[2]/div[2]/ul/li[{page}]/a'\n",
    "                    next_button = driver.find_element(By.XPATH, next_page_xpath)\n",
    "                    next_button.click()\n",
    "                    time.sleep(2)  # Sayfanın yüklenmesi için bekle\n",
    "                except Exception as next_error:\n",
    "                    print(f\"Error navigating to page {page}: {next_error}\")\n",
    "                    break\n",
    "\n",
    "        # \"Sonraki\" düğmesine bas\n",
    "        try:\n",
    "            next_group_xpath = '/html/body/div/div[2]/div[2]/div[2]/ul/li[12]/a'  # \"Sonraki\" düğmesi XPath\n",
    "            next_group_button = driver.find_element(By.XPATH, next_group_xpath)\n",
    "            next_group_button.click()\n",
    "            time.sleep(3)  # Yeni sayfanın yüklenmesini bekle\n",
    "        except Exception as next_group_error:\n",
    "            print(f\"Error clicking 'Sonraki' button: {next_group_error}\")\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during scraping: {e}\")\n",
    "\n",
    "# Yeni veriyi DataFrame'e çevirme\n",
    "new_data = pd.DataFrame(all_data, columns=[\"Name and Title\", \"Department\"])\n",
    "\n",
    "# Mevcut Excel dosyasını okuma\n",
    "file_path = r\"C:\\Users\\alibaki.turkoz\\Desktop\\İzmir_Yüksek_Teknoloji_University_academic_staff.xlsx\"\n",
    "existing_data = pd.read_excel(file_path)\n",
    "\n",
    "# Yeni veriyi mevcut veriye ekleme\n",
    "updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "\n",
    "# Excel dosyasını güncelleme\n",
    "updated_data.to_excel(file_path, index=False, header=True)\n",
    "\n",
    "print(f\"All data has been appended to {file_path}\")\n",
    "\n",
    "# Tarayıcıyı kapat\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page group...\n",
      "Processing page 3...\n",
      "Processing page 4...\n",
      "Processing page 5...\n",
      "Processing page 6...\n",
      "Processing page 7...\n",
      "Processing page 8...\n",
      "Processing page 9...\n",
      "Processing page 10...\n",
      "Processing page 11...\n",
      "Processing page 12...\n",
      "Processing page group...\n",
      "Processing page 3...\n",
      "Error navigating to page 3: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div/div[2]/div[2]/div[2]/ul/li[3]/a\"}\n",
      "  (Session info: chrome=131.0.6778.86); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF713A96CB5+28821]\n",
      "\t(No symbol) [0x00007FF713A03840]\n",
      "\t(No symbol) [0x00007FF7138A578A]\n",
      "\t(No symbol) [0x00007FF7138F91BE]\n",
      "\t(No symbol) [0x00007FF7138F94AC]\n",
      "\t(No symbol) [0x00007FF713942647]\n",
      "\t(No symbol) [0x00007FF71391F33F]\n",
      "\t(No symbol) [0x00007FF71393F412]\n",
      "\t(No symbol) [0x00007FF71391F0A3]\n",
      "\t(No symbol) [0x00007FF7138EA778]\n",
      "\t(No symbol) [0x00007FF7138EB8E1]\n",
      "\tGetHandleVerifier [0x00007FF713DCFCAD+3408013]\n",
      "\tGetHandleVerifier [0x00007FF713DE741F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF713DDB5FD+3455453]\n",
      "\tGetHandleVerifier [0x00007FF713B5BDBB+835995]\n",
      "\t(No symbol) [0x00007FF713A0EB5F]\n",
      "\t(No symbol) [0x00007FF713A0A814]\n",
      "\t(No symbol) [0x00007FF713A0A9AD]\n",
      "\t(No symbol) [0x00007FF7139FA199]\n",
      "\tBaseThreadInitThunk [0x00007FF99A717374+20]\n",
      "\tRtlUserThreadStart [0x00007FF99B8DCC91+33]\n",
      "\n",
      "Error clicking 'Sonraki' button: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div/div[2]/div[2]/div[2]/ul/li[12]/a\"}\n",
      "  (Session info: chrome=131.0.6778.86); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF713A96CB5+28821]\n",
      "\t(No symbol) [0x00007FF713A03840]\n",
      "\t(No symbol) [0x00007FF7138A578A]\n",
      "\t(No symbol) [0x00007FF7138F91BE]\n",
      "\t(No symbol) [0x00007FF7138F94AC]\n",
      "\t(No symbol) [0x00007FF713942647]\n",
      "\t(No symbol) [0x00007FF71391F33F]\n",
      "\t(No symbol) [0x00007FF71393F412]\n",
      "\t(No symbol) [0x00007FF71391F0A3]\n",
      "\t(No symbol) [0x00007FF7138EA778]\n",
      "\t(No symbol) [0x00007FF7138EB8E1]\n",
      "\tGetHandleVerifier [0x00007FF713DCFCAD+3408013]\n",
      "\tGetHandleVerifier [0x00007FF713DE741F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF713DDB5FD+3455453]\n",
      "\tGetHandleVerifier [0x00007FF713B5BDBB+835995]\n",
      "\t(No symbol) [0x00007FF713A0EB5F]\n",
      "\t(No symbol) [0x00007FF713A0A814]\n",
      "\t(No symbol) [0x00007FF713A0A9AD]\n",
      "\t(No symbol) [0x00007FF7139FA199]\n",
      "\tBaseThreadInitThunk [0x00007FF99A717374+20]\n",
      "\tRtlUserThreadStart [0x00007FF99B8DCC91+33]\n",
      "\n",
      "All data has been saved to C:\\Users\\alibaki.turkoz\\Desktop\\İzmir_Yaşar_University_academic_staff.xlsx\n"
     ]
    }
   ],
   "source": [
    "### 1 kez ileri basman gerekiyor, sonra 11. sayfadan itibaren kendisi tüm verileri çekmeye başlar.\n",
    "## İLK EXCELDEN FARKLI EXCEL AÇMAK GEREKİYOR\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Chrome seçeneklerini ayarlayın\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(r\"--user-data-dir=C:\\Users\\alibaki.turkoz\\AppData\\Local\\Google\\Chrome\\User Data\\Default\")\n",
    "options.add_argument(\"--profile-directory=Profile 2\")\n",
    "\n",
    "# Chrome WebDriver başlatma\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Başlangıç URL'si\n",
    "url = \"https://akademik.yok.gov.tr/AkademikArama/view/universityListview.jsp\"\n",
    "\n",
    "# URL'yi açalım\n",
    "driver.get(url)\n",
    "time.sleep(2)  # Sayfanın yüklenmesi için bekleyin\n",
    "\n",
    "# İkinci URL'ye geçiş yapmak için düğmeye tıklama\n",
    "try:\n",
    "    second_url_xpath = '/html/body/div/div[2]/div/table/tbody/tr[124]/td[1]/a'  # Düğme XPath\n",
    "    button = driver.find_element(By.XPATH, second_url_xpath)\n",
    "    button.click()\n",
    "    time.sleep(3)  # Yeni sayfanın yüklenmesini bekleyin\n",
    "except Exception as e:\n",
    "    print(f\"Error clicking the button to navigate: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Tüm sayfalardan sonuçları toplamak için liste\n",
    "all_data = []\n",
    "\n",
    "try:\n",
    "    # Sayfa yüklenmeden önce 3 kez \"Sonraki\" düğmesine bas\n",
    "    for _ in range(0):  # 3 kez \"Sonraki\" düğmesine bas\n",
    "        try:\n",
    "            next_group_xpath = '/html/body/div/div[2]/div[2]/div[2]/ul/li[12]/a'  # \"Sonraki\" düğmesi XPath\n",
    "            next_group_button = driver.find_element(By.XPATH, next_group_xpath)\n",
    "            next_group_button.click()\n",
    "            time.sleep(3)  # Yeni sayfanın yüklenmesini bekle\n",
    "        except Exception as next_group_error:\n",
    "            print(f\"Error clicking 'Sonraki' button: {next_group_error}\")\n",
    "            break\n",
    "\n",
    "    # 12 döngü boyunca sayfalama işlemi yap\n",
    "    for _ in range(15):  # Bu döngü 12 kez tekrar edecek\n",
    "        print(f\"Processing page group...\")\n",
    "\n",
    "        # Sayfa 3'ten 11'e kadar olan sayfaları gez\n",
    "        for page in range(3, 13):  # 3'ten 11'e kadar olan sayfalar\n",
    "            print(f\"Processing page {page}...\")\n",
    "\n",
    "            # Sayfadaki tüm yazarların bilgilerini bul\n",
    "            rows = driver.find_elements(By.XPATH, '//tr[starts-with(@id, \"authorInfo_\")]')\n",
    "\n",
    "            for row in rows:\n",
    "                try:\n",
    "                    # Unvan XPath\n",
    "                    title_xpath = './/td[3]/h6[1]'\n",
    "                    title = row.find_element(By.XPATH, title_xpath).text.strip()\n",
    "\n",
    "                    # İsim XPath\n",
    "                    name_xpath = './/td[3]/h4/a'\n",
    "                    name = row.find_element(By.XPATH, name_xpath).text.strip()\n",
    "\n",
    "                    # Bölüm XPath\n",
    "                    department_xpath = './/td[3]/h6[2]'\n",
    "                    try:\n",
    "                        department_text = row.find_element(By.XPATH, department_xpath).text.strip()\n",
    "                        department = f\"{department_text}\"\n",
    "                    except:\n",
    "                        department = ''  # Bölüm yoksa boş bırak\n",
    "\n",
    "                    # Veriyi listeye ekle\n",
    "                    all_data.append([f\"{title} {name}\", department])\n",
    "\n",
    "                except Exception as row_error:\n",
    "                    print(f\"Error processing row: {row_error}\")\n",
    "\n",
    "            # Sayfayı ilerlet\n",
    "            if page < 12:  # Eğer son sayfa değilse\n",
    "                try:\n",
    "                    next_page_xpath = f'/html/body/div/div[2]/div[2]/div[2]/ul/li[{page}]/a'\n",
    "                    next_button = driver.find_element(By.XPATH, next_page_xpath)\n",
    "                    next_button.click()\n",
    "                    time.sleep(2)  # Sayfanın yüklenmesi için bekle\n",
    "                except Exception as next_error:\n",
    "                    print(f\"Error navigating to page {page}: {next_error}\")\n",
    "                    break\n",
    "\n",
    "        # \"Sonraki\" düğmesine bas\n",
    "        try:\n",
    "            next_group_xpath = '/html/body/div/div[2]/div[2]/div[2]/ul/li[12]/a'  # \"Sonraki\" düğmesi XPath\n",
    "            next_group_button = driver.find_element(By.XPATH, next_group_xpath)\n",
    "            next_group_button.click()\n",
    "            time.sleep(3)  # Yeni sayfanın yüklenmesini bekle\n",
    "        except Exception as next_group_error:\n",
    "            print(f\"Error clicking 'Sonraki' button: {next_group_error}\")\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during scraping: {e}\")\n",
    "\n",
    "# Veriyi DataFrame'e çevirme\n",
    "df = pd.DataFrame(all_data, columns=[\"Name and Title\", \"Department\"])\n",
    "\n",
    "# Excel dosyasına kaydetme\n",
    "file_path = r\"C:\\Users\\alibaki.turkoz\\Desktop\\İzmir_Yüksek_Teknoloji_University_academic_staff.xlsx\"\n",
    "df.to_excel(file_path, index=False, header=True)\n",
    "\n",
    "print(f\"All data has been saved to {file_path}\")\n",
    "\n",
    "# Tarayıcıyı kapat\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACADEMİC STAFF XLSX DOSYASINI SÜTUNLARI DÜZENLEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel dosyası başarıyla güncellendi: C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ - Kopya\\İzmir\\İzmir Yüksek Teknoloji Üniversitesi\\İzmir_Yüksek_Teknoloji_University_academic_staff.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Excel dosyasını yükleyin\n",
    "dosya_adi = r\"C:\\Users\\alibaki.turkoz\\Desktop\\CS552_Project_Ali_Baki_TÜRKÖZ - Kopya\\İzmir\\İzmir Yüksek Teknoloji Üniversitesi\\İzmir_Yüksek_Teknoloji_University_academic_staff.xlsx\"  # Dosya yolu\n",
    "df = pd.read_excel(dosya_adi)\n",
    "\n",
    "# İkinci sütundaki bilgileri '/' ile ayır ve ilk üç kısmı al\n",
    "df[['University', 'Faculty', 'Department']] = df.iloc[:, 1].str.split('/', n=3, expand=True).iloc[:, :3]\n",
    "\n",
    "# Türkçe karakterlere uygun baş harfi büyük, diğerleri küçük formatlama fonksiyonu\n",
    "def format_turkish_text(text):\n",
    "    if isinstance(text, str):  # Sadece metinler için düzenleme yap\n",
    "        return ' '.join([word.capitalize() for word in text.strip().split()])  # Her kelimenin ilk harfi büyük\n",
    "    return text\n",
    "\n",
    "# Sütunları formatla\n",
    "df['University'] = df['University'].apply(format_turkish_text)\n",
    "df['Faculty'] = df['Faculty'].apply(format_turkish_text)\n",
    "df['Department'] = df['Department'].apply(format_turkish_text)\n",
    "\n",
    "# İlk sütunu da baş harf büyük, diğer harf küçük olacak şekilde düzenle\n",
    "df['Title, Name and Surname'] = df.iloc[:, 0].apply(format_turkish_text)\n",
    "\n",
    "# Sütunları yeniden düzenle: University, Faculty, Department, Title, Name and Surname\n",
    "df = df[['University', 'Faculty', 'Department', 'Title, Name and Surname']]\n",
    "\n",
    "# Düzenlenmiş dosyayı aynı isimle kaydet\n",
    "df.to_excel(dosya_adi, index=False)\n",
    "\n",
    "print(f\"Excel dosyası başarıyla güncellendi: {dosya_adi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
